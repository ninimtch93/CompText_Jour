blkstories_bigrams_cts2 <- blkstories_bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigram_cts2
blkstories_bigrams_filtered <- blkstories_bigrams_sep %>%
filter(!word1%in% stop_words$words) %>%
filter(!word2%in% stop_words$words)
blkstories_bigrams_cts2 <- blkstories_bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
slice_max(n, n=20)
blkstories_bigrams_filtered <- blkstories_bigrams_sep %>%
filter(!word1%in% stop_words$words) %>%
filter(!word2%in% stop_words$words)
blkstories_bigrams_cts2 <- blkstories_bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigram_cts2
View(blkstories_bigrams_filtered)
View(blkstories_bigrams_cts2)
View(BlackPressArticles)
View(x)
Blkstories <- str_replace_all(BlackPressArticles$sentence, "- ", "")
Blkstories_df1 <- tibble(Blkstories)
Blkstories_tokenized <- Blkstories_df1 %>%
unnest_tokens(word, Blkstories)
data(stop_words)
Blkstories_tokenized <- Blkstories_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
blkstory_word_ct <- Blkstories_tokenized %>%
count(word, sort=TRUE)
blkstory_word_ct <- Blkstories_tokenized %>%
count(word, sort=TRUE)
blkstories_bigrams <- Blkstories_df1 %>%
unnest_tokens(bigram, Blkstories, token="ngrams", n=2)
blkstories_bigrams_sep <- blkstories_bigrams %>%
separate(bigram, c("word1", "word2"), sep = )
Blkstories_bigram_cts <- blkstories_bigrams %>%
count(bigram, sort = TRUE)
blkstories_bigrams_filtered <- blkstories_bigrams_sep %>%
filter(!word1%in% stop_words$words) %>%
filter(!word2%in% stop_words$words)
blkstories_bigrams_cts2 <- blkstories_bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigram_cts2
View(blkstory_word_ct)
blkstories_bigrams_filtered <- blkstories_bigrams_sep %>%
filter(!word1%in% stop_words$words) %>%
filter(!word2%in% stop_words$words)
View(blkstories_bigrams_filtered)
blkstories_bigrams_cts2 <- blkstories_bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigram_cts2
blkstories_bigrams_cts2 <- blkstories_bigrams %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
blkstories_bigrams_cts2 <- Blkstories_bigrams %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
blkstories_bigrams_cts2 <- blkstories_bigrams %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
blkstories_bigrams_cts2 <- blkstories_bigrams_sep %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigram_cts2
#install.packages("tidyverse")
#install.packages("textdata")
#install.packages("tidytext")
#install.packages("quanteda")
#install.packages("rio")
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
#import df created from sequence below
lynch <- read.csv("../data/articles_oct_19.csv")
#update to the file path for your code folder
#black <- read.csv("~/Code/CompText_Jour/data/black_press_extracted_text_june_22_2024.csv")
all_text <- str_replace_all(lynch$sentence, "- ", "")
text_df <- tibble(all_text,)
# unnest includes lower, punct removal
text_tokenized <- text_df %>%
unnest_tokens(word,all_text)
text_tokenized
#Remove stopwords
data(stop_words)
text_tokenized<- text_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
#NOT SURE IF THIS LINE SHOULD REMAIN
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
# fix the script so it doesn't pick up these file names, numbers
# forcibly removing for now
# Word Count
text_word_ct <- text_tokenized %>%
count(word, sort=TRUE)
# cite this lexicon
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
#load tidyverse, tidytext, rio and quanteda libraries
#install.packages("tidyverse")
#install.packages("tidyterra")
#install.packages("rio")
#install.packages("quanteda")
library(tidyverse)
library(tidytext)
library(rio)
library(quanteda)
#Import dataframe
lynch <- read_csv("~/CompText_Jour-main/data/articles_oct_19.csv")
Years <- lynch %>%
filter(year >= 1900 & year <= 1910)
Years
distinctarticles <- Years %>%
distinct(filename, .keep_all = TRUE) %>%
count(year)
distinctarticles
NewsStates <- Years %>%
count(year, newspaper_state)
NewsStates
stories1 <- str_replace_all(Years$sentence, "- ", "")
stories_df1 <- tibble(stories1)
data(stop_words)
test <- stop_words %>%
as.data.frame()
head(test)
stories_bigrams <- stories_df1 %>%
unnest_tokens(bigram, stories1, token="ngrams", n=2)
stories_bigrams_separated <- stories_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
stories_bigram_cts <- stories_bigrams %>%
count(bigram, sort = TRUE)
stories_bigrams_filtered <- stories_bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
stories_bigram_cts2 <- stories_bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
stories_bigram_cts2
stories_bigram_cts_1900 <- stories_bigram_cts %>%
mutate(decade = "1900-1910")
BlackPressArticles <- lynch %>%
filter(black_press == "Y")
BlackPressArticles
Blkstories <- str_replace_all(BlackPressArticles$sentence, "- ", "")
Blkstories_df1 <- tibble(Blkstories)
Blkstories_tokenized <- Blkstories_df1 %>%
unnest_tokens(word, Blkstories)
#install.packages("tidyverse")
#install.packages("textdata")
#install.packages("tidytext")
#install.packages("quanteda")
#install.packages("rio")
#install.packages("scico")
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(scico)
#import df created from sequence below
lynch <- read.csv("../data/articles_oct_19.csv")
#update to the file path for your code folder
#black <- read.csv("~/Code/CompText_Jour/data/black_press_extracted_text_june_22_2024.csv")
all_text <- str_replace_all(lynch$sentence, "- ", "")
text_df <- tibble(all_text,)
# unnest includes lower, punct removal
text_tokenized <- text_df %>%
unnest_tokens(word,all_text)
text_tokenized
#Remove stopwords
data(stop_words)
text_tokenized<- text_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
#NOT SURE IF THIS LINE SHOULD REMAIN
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
# fix the script so it doesn't pick up these file names, numbers
# forcibly removing for now
# Word Count
text_word_ct <- text_tokenized %>%
count(word, sort=TRUE)
# cite this lexicon
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")
nrc_sentiments %>% count(sentiment)
#sentiment & count
# anger	1246
# anticipation	837
# disgust	1056
# fear	1474
# joy	687
# negative	3318
# positive	2308
# sadness	1187
# surprise	532
# trust	1230
nrc_sentiments %>%
group_by(word) %>%
count() %>%
arrange(desc(n)) %>%
distinct()
sentiments_all <- text_tokenized %>%
inner_join(nrc_sentiments)
#this dictionary assigns different values to the same word. negro is negative, sadness whereas lynch is anger, disgust, fear, negative and sadness.
x <- sentiments_all %>%
group_by(word) %>%
count(sentiment)
sentiments_all <- text_tokenized %>%
inner_join(nrc_sentiments) %>%
count(sentiment, sort = TRUE) %>%
mutate(pct_total =round(n/sum(n), digits=2))
sentiments_all
library(ggplot2)
afinn_plot <- ggplot(sentiments_all,aes(x = sentiment, y = n,fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Total Sentiment in Black Press Lynching News Coverage",
subtitle = " ",
caption = "NRC Sentiment analysis. Graphic by Rob Wells, 8-25-2024",
y="Score",
x="total sentiment score")
afinn_plot + scico::scale_fill_scico(palette = "vik")
# ggsave("Figure5_afinn_sentiment_jan2.png",device = "png",width=9,height=6, dpi=800)
# Anger
nrc_anger <- nrc_sentiments %>%
filter(sentiment == "anger")
lynching_anger <- text_tokenized %>%
inner_join(nrc_anger) %>%
count(word, sort = TRUE)
lynching_anger
# Anticipation
# results / themes not as clear as anger
nrc_anticipation <- nrc_sentiments %>%
filter(sentiment == "anticipation")
lynching_anticipation <-text_tokenized%>%
inner_join(nrc_anticipation) %>%
count(word, sort = TRUE)
lynching_anticipation
# Fear
# see a reflection of the basic word count in these results
nrc_fear <- nrc_sentiments %>%
filter(sentiment == "fear")
lynching_fear <-text_tokenized%>%
inner_join(nrc_fear) %>%
count(word, sort = TRUE)
lynching_fear
# Disgust
# see a reflection of the basic word count in these results
nrc_disgust <- nrc_sentiments %>%
filter(sentiment == "disgust")
lynching_disgust <-text_tokenized%>%
inner_join(nrc_disgust) %>%
count(word, sort = TRUE)
lynching_disgust
#We should do our own custom sentiment dictionary based on the top 500 words.
text_500 <- text_word_ct %>%
filter(n >= 29)
custom_dictionary <- text_500 %>%
inner_join(nrc_sentiments)
#write.csv(custom_dictionary, "custom_dictionary.csv")
#before 1870
pre1870 <- lynch %>%
filter(year < 1870)
#1870-1879
the1870s <-  lynch %>%
filter(year >= 1870 & year <=1879)
#1880-1889
the1880s <-  lynch %>%
filter(year >= 1880 & year <=1889)
#1890-1899
the1890s <-  lynch %>%
filter(year >= 1890 & year <=1899)
#1900-1909
the1900s <-  lynch %>%
filter(year >= 1900 & year <=1909)
#1910-1919
the1910s <-  lynch %>%
filter(year >= 1910 & year <=1919)
#1920-1929
the1920s <-  lynch %>%
filter(year >= 1920 & year <=1929)
#1930-1960
post1930s <-  lynch %>%
filter(year >= 1930)
lynch_decade <- lynch %>%
mutate(decade = case_when(
year < 1870 ~ "pre1870",
year >= 1870 & year <=1879 ~ "1870s",
year >= 1880 & year <=1889 ~ "1880s",
year >= 1890 & year <=1899 ~ "1890s",
year >= 1900 & year <=1909 ~ "1900s",
year >= 1910 & year <=1919 ~ "1910s",
year >= 1920 & year <=1929 ~ "1920s",
year >= 1930 ~ "post1930s"
))
lynch_decade <- lynch %>%
mutate(decade = case_when(
year < 1870 ~ "pre1870",
year >= 1870 & year <=1879 ~ "1870s",
year >= 1880 & year <=1889 ~ "1880s",
year >= 1890 & year <=1899 ~ "1890s",
year >= 1900 & year <=1909 ~ "1900s",
year >= 1910 & year <=1919 ~ "1910s",
year >= 1920 & year <=1929 ~ "1920s",
year >= 1930 ~ "post1930s"
))
# lynch_decade %>%
#   count(newspaper_state)
lynch_decade <- lynch_decade %>%
mutate(region=newspaper_state) %>%
mutate(region = case_when(region=="South Carolina" ~ "South",
region=="Texas" ~ "South",
region=="Louisiana" ~ "South",
region=="Tennessee" ~ "South",
region=="Mississippi" ~ "South",
region=="Arkansas" ~ "South",
region=="Alabama" ~ "South",
region=="Georgia" ~ "South",
region=="Virginia" ~ "South",
region=="Florida" ~ "South",
region=="North Carolina" ~ "South",
region=="Maryland" ~ "Border",
region=="Delaware" ~ "Border",
region=="West Virginia" ~ "Border",
region=="Kentucky" ~ "Border",
region=="Missouri" ~ "Border",
region=="Maine" ~ "North",
region=="New York" ~ "North",
region=="New Hampshire" ~ "North",
region=="Vermont" ~ "North",
region=="Massassachusetts" ~ "North",
region=="Connecticut" ~ "North",
region=="Rhode Island" ~ "North",
region=="Pennsylvania" ~ "North",
region=="New Jersey" ~ "North",
region=="Ohio" ~ "North",
region=="Indiana" ~ "North",
region=="Kansas" ~ "North",
region=="Michigan" ~ "North",
region=="Wisconsin" ~ "North",
region=="Minnesota" ~ "North",
region=="Iowa" ~ "North",
region=="California" ~ "North",
region=="Nevada" ~ "North",
region=="Oregon" ~ "North",
region=="Illinois" ~ "North",
region=="Nebraska" ~ "Misc",
region=="Colorado" ~ "Misc",
region=="North Dakota" ~ "Misc",
region=="South Dakota" ~ "Misc",
region=="Montana" ~ "Misc",
region=="Washington" ~ "Misc",
region=="Idaho" ~ "Misc",
region=="Wyoming" ~ "Misc",
region=="Utah" ~ "Misc",
region=="Oklahoma" ~ "Misc",
region=="New Mexico" ~ "Misc",
region=="Arizona" ~ "Misc",
region=="Alaska" ~ "Misc",
region=="Hawaii" ~ "Misc",
region=="District of Columbia" ~ "Misc",
region=="Virgin Islands" ~ "Misc",
TRUE~region))
sentiments_south <- sentiments_south %>%
rename(south_n = n, south_pct = pct_total)
View(nrc_sentiments)
#install.packages("tidyverse")
#install.packages("textdata")
#install.packages("tidytext")
#install.packages("quanteda")
#install.packages("rio")
#install.packages("scico")
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(scico)
#import df created from sequence below
lynch <- read.csv("../data/articles_oct_19.csv")
#update to the file path for your code folder
#black <- read.csv("~/Code/CompText_Jour/data/black_press_extracted_text_june_22_2024.csv")
all_text <- str_replace_all(lynch$sentence, "- ", "")
text_df <- tibble(all_text,)
# unnest includes lower, punct removal
text_tokenized <- text_df %>%
unnest_tokens(word,all_text)
text_tokenized
#Remove stopwords
data(stop_words)
text_tokenized<- text_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
#NOT SURE IF THIS LINE SHOULD REMAIN
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
# fix the script so it doesn't pick up these file names, numbers
# forcibly removing for now
# Word Count
text_word_ct <- text_tokenized %>%
count(word, sort=TRUE)
View(text_tokenized)
View(text_word_ct)
# cite this lexicon
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")
nrc_sentiments %>% count(sentiment)
#sentiment & count
# anger	1246
# anticipation	837
# disgust	1056
# fear	1474
# joy	687
# negative	3318
# positive	2308
# sadness	1187
# surprise	532
# trust	1230
nrc_sentiments %>%
group_by(word) %>%
count() %>%
arrange(desc(n)) %>%
distinct()
View(nrc_sentiments)
View(afinn_sentiments)
# cite this lexicon
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")
nrc_sentiments %>% count(sentiment)
#sentiment & count
# anger	1246
# anticipation	837
# disgust	1056
# fear	1474
# joy	687
# negative	3318
# positive	2308
# sadness	1187
# surprise	532
# trust	1230
nrc_sentiments %>%
group_by(word) %>%
count() %>%
arrange(desc(n)) %>%
distinct()
sentiments_all <- text_tokenized %>%
inner_join(nrc_sentiments)
#this dictionary assigns different values to the same word. negro is negative, sadness whereas lynch is anger, disgust, fear, negative and sadness.
x <- sentiments_all %>%
group_by(word) %>%
count(sentiment)
sentiments_all <- text_tokenized %>%
inner_join(nrc_sentiments)
#this dictionary assigns different values to the same word. negro is negative, sadness whereas lynch is anger, disgust, fear, negative and sadness.
dropped <- sentiments_all %>%
group_by(word) %>%
count(sentiment)
sentiments_all <- text_tokenized %>%
inner_join(nrc_sentiments)
#this dictionary assigns different values to the same word. negro is negative, sadness whereas lynch is anger, disgust, fear, negative and sadness.
dropped <- sentiments_all %>%
anti_join(nrc_sentiments)
View(dropped)
View(sentiments_all)
sentiments_all <- text_tokenized %>%
inner_join(nrc_sentiments)
#this dictionary assigns different values to the same word. negro is negative, sadness whereas lynch is anger, disgust, fear, negative and sadness.
dropped <- text_tokenized %>%
anti_join(nrc_sentiments)
View(dropped)
View(sentiments_all)
sentiments_all <- text_tokenized %>%
inner_join(nrc_sentiments) %>%
count(sentiment, sort = TRUE) %>%
mutate(pct_total =round(n/sum(n), digits=2))
sentiments_all
View(sentiments_all)
library(ggplot2)
afinn_plot <- ggplot(sentiments_all,aes(x = sentiment, y = n,fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Total Sentiment in Black Press Lynching News Coverage",
subtitle = " ",
caption = "NRC Sentiment analysis. Graphic by Rob Wells, 8-25-2024",
y="Score",
x="total sentiment score")
afinn_plot + scico::scale_fill_scico(palette = "vik")
# ggsave("Figure5_afinn_sentiment_jan2.png",device = "png",width=9,height=6, dpi=800)
# Anger
nrc_anger <- nrc_sentiments %>%
filter(sentiment == "anger")
lynching_anger <- text_tokenized %>%
inner_join(nrc_anger) %>%
count(word, sort = TRUE)
lynching_anger
View(nrc_anger)
library(tidyverse)
library(pdftools)
#install.packages("pdftools")
