---
title: "Final Project Draft - Nini Mtchedlishvili"
output: html_document
date: "2024-11-12"
---

Currently, I have collected 29 articles from the Daily Wire, a prominent right-wing media organization in the United States. These articles are part of a larger research project aimed at analyzing how this outlet portrays two political figures: Kamala Harris and Donald Trump. The project focuses on articles published a few days before recent elections, as this period is likely to reveal editorial patterns and biases in politically charged reporting. My immediate goal is to expand the dataset to 100 articles about each candidate to ensure thorough comparative analysis.

The core of this research lies in examining the language and sentiment employed by the Daily Wire when referring to Kamala Harris versus Donald Trump. I aim to identify patterns in tone, word choice, and framing. Sentiment analysis will help determine whether the portrayal of Harris skews more negative or positive compared to Trump, while textual comparisons will highlight recurring themes and rhetorical strategies.

Ultimately, this project seeks to uncover potential biases in the Daily Wire's reporting. If disparities in sentiment and language are evident, it could provide valuable insights into the media outlet's approach to covering candidates of opposing political affiliations. By shedding light on these patterns, the study will contribute to broader conversations about media bias and its influence on public opinion during critical electoral periods.

The links to the dataset: https://github.com/ninimtch93/CompText_Jour/blob/main/FPD_Nini/Daily_Wire_Articles.xlsx


```{r}

# install.packages("tidyverse")
# install.packages("rvest")
# install.packages("janitor")

library(tidyverse)
library(rvest)
library(janitor)
library(readxl)
library(ggplot2)


```

```{r}

# Define the function to scrape a Daily Wire article
scrape_article <- function(url) {
  # Read the HTML content of the page
  page <- tryCatch(
    read_html(url),
    error = function(e) {
      message(paste("Error reading:", url))
      return(NULL)
    }
  )
  
  # Return NULL if page could not be loaded
  if (is.null(page)) return(NULL)

  # Extract the headline (assuming it's in an <h1> tag)
  headline <- page %>%
    html_element("h1") %>%
    html_text(trim = TRUE)

  # Extract the article text (all <p> elements)
  article_text <- page %>%
    html_elements("p") %>%
    html_text(trim = TRUE) %>%
    paste(collapse = "\n")
  
  # Extract the publication date (assuming it's in a <time> or meta tag)
  date <- page %>%
    html_element("time") %>%
    html_attr("datetime") # Extract datetime attribute if available
  
  # If <time> is not used, try a common meta tag for publication date
  if (is.na(date) || is.null(date)) {
    date <- page %>%
      html_element("meta[property='article:published_time']") %>%
      html_attr("content") # Extract content attribute
  }
  
  # If still no date, assign NA
  if (is.na(date) || is.null(date)) {
    date <- NA
  }

  # Combine headline, date, and article into one string for reference
  full_text <- paste("Headline:\n", headline, "\n\nDate:\n", date, "\n\nArticle Text:\n", article_text)
  
  return(list(full_text = full_text, date = date))
}

# Read the URLs from the Excel file
urls <- read_excel("Daily_Wire_Articles.xlsx", sheet = 1)$URL

# Specify the folder for saving articles
output_folder <- "extracted_text"

# Ensure the folder exists (create it if it doesn't)
if (!dir.exists(output_folder)) {
  dir.create(output_folder)
}

# Initialize a data frame to store article metadata
article_metadata <- data.frame(
  URL = character(),
  Date = character(),
  stringsAsFactors = FALSE
)

# Loop over URLs and save each article as a text file
for (url in urls) {
  article_data <- scrape_article(url)
  
  # Skip if article content is NULL
  if (is.null(article_data)) next

  # Extract the content and date
  article_content <- article_data$full_text
  article_date <- article_data$date
  
  # Create a filename from the URL (sanitize to avoid invalid characters)
  filename <- paste0(gsub("[^a-zA-Z0-9]", "_", basename(url)), ".txt")
  
  # Save the article content to a text file in the specified folder
  file_path <- file.path(output_folder, filename)
  writeLines(article_content, con = file_path)
  
  # Append metadata to the data frame
  article_metadata <- rbind(article_metadata, data.frame(URL = url, Date = article_date))
  
  message(paste("Saved:", file_path))
}


```


```{r}
library(rvest)

# Define the URL
url2 <- "https://www.dailywire.com/news/not-encouraging-nyt-warns-kamala-in-trouble-as-poll-finds-her-tied-with-trump?topStoryPosition=1"

# Read the HTML content of the page
page <- read_html(url2)

# Extract the headline (usually in an <h1> tag)
headline <- page %>%
  html_element("h1") %>%
  html_text(trim = TRUE)

# Extract the article text (all <p> elements)
article_text <- page %>%
  html_elements("p") %>%
  html_text(trim = TRUE)

# Print the headline and article content
cat("Headline:\n", headline, "\n\n")
cat("Article Text:\n", paste(article_text, collapse = "\n"))

```


in R using this example build a function to take individual URLs from a data fram called urls and compile the extracted text into dataframe
```{r}
urls_list <- urls %>% 
  as.data.frame() %>% 
  rename(site = 1)


```

```{r}
# Define the function to extract text and date from a limited set of URLs in urls_list
extract_text_from_head_urls <- function(urls_list, n = 29) {
  # Select the first 'n' URLs from the urls_list data frame
  urls_to_process <- head(urls_list$site, n)
  
  # Initialize an empty data frame to store results
  results_df <- data.frame(
    url = character(),
    headline = character(),
    article_text = character(),
    date = character(),
    stringsAsFactors = FALSE
  )
  
  # Loop through each URL in the selected URLs
  for (url in urls_to_process) {
    # Read the HTML content of the page
    page <- tryCatch({
      read_html(url)
    }, error = function(e) {
      message("Error reading URL: ", url)
      return(NULL)
    })
    
    # Skip if page could not be read
    if (is.null(page)) next
    
    # Extract the headline
    headline <- page %>%
      html_element("h1") %>%
      html_text(trim = TRUE)
    
    # Extract the article text (all <p> elements)
    article_text <- page %>%
      html_elements("p") %>%
      html_text(trim = TRUE) %>%
      paste(collapse = "\n")
    
    # Extract the publication date
    date <- page %>%
      html_element("time") %>%
      html_attr("datetime") # Extract datetime attribute if available
    
    # If <time> is not used, try a common meta tag for publication date
    if (is.na(date) || is.null(date)) {
      date <- page %>%
        html_element("meta[property='article:published_time']") %>%
        html_attr("content") # Extract content attribute
    }
    
    # If still no date, assign NA
    if (is.na(date) || is.null(date)) {
      date <- NA
    }
    
    # Append the results to the data frame
    results_df <- results_df %>%
      add_row(url = url, headline = headline, article_text = article_text, date = date)
  }
  
  return(results_df)
}

# Example usage with your urls data frame
# Assuming urls_list has a column 'site' containing the URLs
# urls_list <- data.frame(site = c("https://example.com/article1", "https://example.com/article2"))

compiled_text_df <- extract_text_from_head_urls(urls_list, n = 29)

# View the compiled results
print(compiled_text_df)


```

Content Analysis Plan

Collecting and Preparing the Data

The first step is to complete the collection of articles about Kamala Harris and Donald Trump from the Daily Wire. Currently, I have 29 articles and aim to gather 100 articles for each candidate. Once the dataset is complete, I will clean and organize the text files saved in the extracted_text folder to ensure they are ready for analysis.



Building and Using My Code Book

Throughout our classes, I have managed to start collecting all of the codes that we went through. Because my project is going to be a comparative analysis of how a right-wing media organization portraied a Republican and a Democrat presidential candidates, using the codes in a right order will hold a crucial role in my research. 

After gathering all of the data, I will analyze the bigrams to compare what two-word phrases were mostly used while talking about Donald Trump and Kamala Harris.

For sentiment analysis, the codes will categorize text into:
Positive: words like "success," "strong," or "leader."
Negative: words like "failure," "weak," or "untrustworthy."
Neutral or mixed: words like "controversial," "debate," or "discussion."

I plan to apply consistent rules for identifying patterns in the articles and assign sentiment scores to compare how each candidate is portrayed.



Analyzing the Data
The analysis will include Text Analysis that will help me in identifying the most frequently used bigrams for each candidate and comparing the themes, and Sentiment Analysis that will allow me to measure the tone (positive, negative, or neutral) toward Kamala Harris and Donald Trump using sentiment lexicons.



Visualization with ggplot2

I plan to present at least to data visualization charts: Bigrams Bar Plot that will show the frequency of top 20 bigrams for each candidate, and
Sentiment Comparison Plot that will visualize the compared sentiment distributions side-by-side for Harris and Trump.

I might also do a Word Cloud: Summarizing frequently used words or phrases visually.



How My Code Book Will Support the Project

Standardizing Research:
My code book will ensure consistency and transparency in coding the data. By defining clear rules for bigrams and sentiment categories, I will apply the same approach to all articles on both sides. This prevents subjective interpretations and makes it easier to compare language and tone across articles about Kamala Harris and Donald Trump.

Organizing Data:
With a large dataset of up to 200 articles, my code book will save time by providing a structured guide for categorizing and analyzing text. Having clear codes for key themes and sentiment ensures the focus remains on relevant patterns, leading to richer and more meaningful results.

Quality Control:
My code book will provide clear definitions and examples for each code, ensuring that data is entered and analyzed consistently. This systematic approach will reduce errors and strengthen the reliability of the findings. This will make it easier to draw valid conclusions about potential biases in the Daily Wire’s reporting.





Sample of Data and Descriptive Statistics

Below you can find the dataframe of the complied articles and the text of each article

```{r}

compiled_text_df

```

Below you can see that the article word-count varies from 391 to 1803. Most of the articles' word-count is in-between 500-900.

```{r}

WordCount <- compiled_text_df %>%
  select(article_text) %>% 
  mutate(WordCount = str_count(article_text, "\\w+")) %>% 
  arrange(desc(WordCount))
  

WordCount

```

Visualization of the word count

```{r}
ggplot(WordCount, aes(x = WordCount)) +
  geom_histogram(binwidth = 50, fill = "blue", color = "black") +
  labs(
    title = "Distribution of Word Counts in Articles",
    x = "Word Count",
    y = "Frequency"
  ) +
  theme_minimal()

```

Code for counting rows

```{r}

nrow(compiled_text_df)

```

Code for counting columns
```{r}

ncol(compiled_text_df)

```

```{r}


# Create the histogram
ggplot(compiled_text_df, aes(x = date)) +
  geom_bar(binwidth = 7, fill = "blue", color = "black") + # Weekly bins
  labs(
    title = "Distribution of Articles about Kamala Harris Over Time",
    caption =  "Graphics by Nini Mtchedlishvili",
    x = "Date",
    y = "Number of Articles"
  ) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

By collecting more data in upcoming days, data analysis will show more. 