

                                                                                                      Page 1 of 3
      University of Washington School of Medicine: Sky-is-Falling Scenarios Distract From Risks AI Poses Today




      University of Washington School of Medicine: Sky-is-Falling Scenarios
                       Distract From Risks AI Poses Today
                                                     Targeted News Service
                                          April 17, 2024 Wednesday 10:15 AM EST



Copyright 2024 Targeted News Service LLC All Rights Reserved




Length: 776 words
Byline: Targeted News Service
Dateline: SEATTLE, Washington

Body


(TNSres) -- The University of Washington's School of Medicine issued the following news release:

***

In a paper, academic ethicists criticize tech leaders and others who emphasize calamity thinking with artificial
intelligence.

***

Tech-industry leaders and spotlight-seeking luminaries are monopolizing global discourse about artificial
intelligence (AI), and often conveying that its adoption carries catastrophic risks beyond humans' comprehension.
Doomsayers claim that AI carries "an adverse outcome so bad (that) it would either annihilate Earth-originating
intelligent life or permanently or drastically curtail its potential."

These projections of distant-future disasters -- and the media reporters who invite them -- do a disservice to the
public by taking the focus away from societal problems that AI is causing today, and from consideration of AI's
benefits.

A group of ethicists argue these points in an open-access article published this month in the Journal of Medical
Ethics. Lead author Nancy Jecker is a professor of bioethics at the University of Washington School of Medicine.

"Existential risk, which we call 'X-Risk,' refers to activities that threaten grave dangers to humanity, like nuclear
weapons, climate change and emerging infectious diseases," she said. "These have tremendous capacity to wipe
out large numbers of people and undermine human well-being. We approach them with a balanced risk assessment
that includes risks occurring today."

With AI, here-and-now concerns involve algorithmic bias leading to gender and racial discrimination, AI-generated
child sexual abuse, labor exploitation, especially in poorer countries, and displacement of human creative work.
Misinformation during this election year is another major concern, Jecker said.

                                                                                                      Page 2 of 3
      University of Washington School of Medicine: Sky-is-Falling Scenarios Distract From Risks AI Poses Today

Media outlets looking for headline clicks are tempted to instead emphasize AI debates that raise the specter of
distant disaster, she suggested. Likewise, technology leaders who have financial stakes in AI's development know
that ratcheting up public fears about far-off calamities can eclipse consideration of AI's present harms.

"What I'm most concerned about is what's not being said and where the spotlight isn't," Jecker offered. "Most
people in the tech industry don't need to personally worry about being declined for a job or a bank loan because of
a sexist or ableist algorithm, or not being considered for parole because of a racist algorithm."

Technology workers, especially leaders, are overwhelmingly white men without disabilities, and this standpoint
informs their ethics assessments, she suggested.

The authors wrote the paper with the hope of broadening AI ethics conversation to include not only technologically
aware voices but also those who are historically marginalized and whose opportunities are at risk.

The paper cited a Stanford University 2023 analysis of scholarly AI ethics literature that saw a shift away from
academic authors and toward authors with tech-industry affiliations; tech authors produced 71% more publications
than academics between 2014 and 2022.

"Tech workers lack formal training in ethics," Jecker said. "They can tell us about choices to be made within AI, but
they shouldn't lead ethics debates in the public square. Not only are tech leaders not trained to do so, but they also
have a conflict of interest, given their work in that industry."

One example of positive direction for AI discourse, Jecker said, is the public-private partnership just announced by
the University of Washington and the University of Tsukuba in Japan, with private sector investment by Amazon,
Nvidia and other companies. The project aims to further research, entrepreneurship, workforce development and
social implementation of artificial intelligence

"We need to engage in cross-border efforts that involve diverse groups from different sectors of society to come
together to work through complex issues," Jecker said.

Catastrophe-fixated comments can also divert attention from AI benefits in areas like medicine, the authors wrote.
They referenced AI's ability to help radiologists identify high-risk patient cases, to advance precision medicine
based on genomic analysis, and to scour large datasets to better predict patient outcomes.

"A balanced approach to AI must weigh benefits as well as risks," Jecker said.

***

JOURNAL: Journal of Medical Ethics https://jme.bmj.com/content/early/2024/04/04/jme-2023-109702.full

***

Original text here: https://newsroom.uw.edu/news-releases/sky-is-falling-scenarios-distract-from-risks-ai-poses-
today

Contact: Brian Donohue, 206/543-7856, bdonohue@uw.edu

Copyright Targeted News Services

T40-MgEditor-8578162 T40-MgEditor

Classification
Language: ENGLISH

                                                                                                    Page 3 of 3
    University of Washington School of Medicine: Sky-is-Falling Scenarios Distract From Risks AI Poses Today


Publication-Type: Newswire


Subject: ETHICS (91%); ARTIFICIAL INTELLIGENCE (90%); NEWS REPORTING (90%); ARTIFICIAL
INTELLIGENCE ETHICS (89%); NEGATIVE NEWS (89%); NEGATIVE SOCIETAL NEWS (89%); RISK
MANAGEMENT (89%); TECHNOLOGY (89%); WRITERS (89%); DISCRIMINATION (87%); GENDER & SEX
DISCRIMINATION (86%); ABUSE & NEGLECT (78%); BIOETHICS (78%); MEDICAL ETHICS (78%); RACISM &
XENOPHOBIA (78%); SEX & GENDER ISSUES (78%); SOCIETAL ISSUES (78%); SOCIETY, SOCIAL
ASSISTANCE & LIFESTYLE (78%); TECHNICIANS & TECHNOLOGICAL WORKERS (78%); GENERATIVE AI
(77%); PUBLIC HEALTH (75%); HUMAN SUBJECTS (74%); COLLEGE & UNIVERSITY PROFESSORS (73%);
DISEASES & DISORDERS (73%); INFECTIOUS DISEASE (73%); PAROLE (73%); PRESS RELEASES (73%);
CAMPAIGNS & ELECTIONS (72%); LIFE FORMS (71%); WORKPLACE HEALTH & SAFETY (70%); RACE &
ETHNICITY (69%); RACIAL DISCRIMINATION IN EMPLOYMENT (69%); RESEARCH REPORTS (69%);
DISINFORMATION & MISINFORMATION (64%); CHILD ABUSE & NEGLECT (63%); NUCLEAR WEAPONS
(53%); MILITARY WEAPONS (51%); WEAPONS & ARMS (51%); CHILD SEXUAL ABUSE (50%)


Organization: UNIVERSITY OF WASHINGTON (94%)


Industry: ARTIFICIAL INTELLIGENCE (90%); NEWS REPORTING (90%); ARTIFICIAL INTELLIGENCE ETHICS
(89%); RISK MANAGEMENT (89%); WRITERS (89%); GENERATIVE AI (77%); INFORMATION TECHNOLOGY
INDUSTRY (77%); COLLEGE & UNIVERSITY PROFESSORS (73%); NUCLEAR WEAPONS (53%); MILITARY
WEAPONS (51%)


Geographic: SEATTLE, WA, USA (79%); WASHINGTON, USA (79%); UNITED STATES (94%)


Load-Date: April 17, 2024


  
